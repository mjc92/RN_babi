{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn,optim\n",
    "from model_debug import RN\n",
    "from functions import from_batch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "batch_size = 64\n",
    "embed_size = 64\n",
    "en_hidden_size = 32\n",
    "mlp_hidden_size = 256\n",
    "epochs = 30\n",
    "\n",
    "# optional: if you're starting off from a previous model\n",
    "startoff=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "with open('train_10k.txt') as f:\n",
    "    lines=f.readlines()\n",
    "if len(lines)%batch_size==0:\n",
    "    num_batches = int(len(lines)/batch_size)\n",
    "else:\n",
    "    num_batches = int(len(lines)/batch_size)\n",
    "\n",
    "# load vocabulary\n",
    "word2idx = np.load('word2idx.npy').item()\n",
    "idx2word = np.load('idx2word.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if startoff>0:\n",
    "    rn = torch.load('saved/rn_%d.pth' %startoff)\n",
    "else:\n",
    "    rn = RN(vocab_size, embed_size, en_hidden_size, mlp_hidden_size)\n",
    "if torch.cuda.is_available():\n",
    "    rn = rn.cuda()\n",
    "opt = optim.Adam(rn.parameters(),lr=2e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for validation\n",
    "def validate(rn, val_set):\n",
    "    S, Q, A = from_batch(val_set)\n",
    "    out = rn(S, Q)\n",
    "    O = torch.max(out,1)[1].cpu().data.numpy().squeeze()\n",
    "    score = np.array(O==A,int)\n",
    "    total = len(score)\n",
    "    correct = sum(score)\n",
    "    return total,correct\n",
    "\n",
    "with open('test_10k.txt') as f:\n",
    "    val_set=f.readlines()[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss for 0/3125: 5.711\n",
      "loss for 50/3125: 4.233\n",
      "loss for 100/3125: 2.874\n",
      "loss for 150/3125: 3.000\n",
      "loss for 200/3125: 2.932\n",
      "loss for 250/3125: 3.333\n",
      "loss for 300/3125: 3.020\n",
      "loss for 350/3125: 2.556\n",
      "loss for 400/3125: 3.063\n",
      "loss for 450/3125: 3.170\n",
      "loss for 500/3125: 2.779\n",
      "loss for 550/3125: 3.032\n",
      "loss for 600/3125: 3.468\n",
      "loss for 650/3125: 2.632\n",
      "loss for 700/3125: 2.778\n",
      "loss for 750/3125: 2.968\n",
      "loss for 800/3125: 2.068\n",
      "loss for 850/3125: 2.882\n",
      "loss for 900/3125: 2.318\n",
      "loss for 950/3125: 2.641\n",
      "loss for 1000/3125: 2.886\n",
      "loss for 1050/3125: 3.174\n",
      "loss for 1100/3125: 2.896\n",
      "loss for 1150/3125: 3.014\n",
      "loss for 1200/3125: 2.641\n",
      "loss for 1250/3125: 2.799\n",
      "loss for 1300/3125: 2.565\n",
      "loss for 1350/3125: 2.971\n",
      "loss for 1400/3125: 2.497\n",
      "loss for 1450/3125: 2.605\n",
      "loss for 1500/3125: 2.826\n",
      "loss for 1550/3125: 3.295\n",
      "loss for 1600/3125: 2.918\n",
      "loss for 1650/3125: 2.729\n",
      "loss for 1700/3125: 2.937\n",
      "loss for 1750/3125: 2.359\n",
      "loss for 1800/3125: 2.824\n",
      "loss for 1850/3125: 3.035\n",
      "loss for 1900/3125: 2.492\n",
      "loss for 1950/3125: 2.809\n",
      "loss for 2000/3125: 2.944\n",
      "loss for 2050/3125: 2.617\n",
      "loss for 2100/3125: 2.548\n",
      "loss for 2150/3125: 2.685\n",
      "loss for 2200/3125: 2.850\n",
      "loss for 2250/3125: 2.773\n",
      "loss for 2300/3125: 2.419\n",
      "loss for 2350/3125: 2.822\n",
      "loss for 2400/3125: 2.942\n",
      "loss for 2450/3125: 2.766\n",
      "loss for 2500/3125: 2.670\n",
      "loss for 2550/3125: 2.720\n",
      "loss for 2600/3125: 3.069\n",
      "loss for 2650/3125: 2.691\n",
      "loss for 2700/3125: 2.281\n",
      "loss for 2750/3125: 2.242\n",
      "loss for 2800/3125: 2.757\n",
      "loss for 2850/3125: 2.376\n",
      "loss for 2900/3125: 3.161\n",
      "loss for 2950/3125: 2.333\n",
      "loss for 3000/3125: 2.733\n",
      "loss for 3050/3125: 2.335\n",
      "loss for 3100/3125: 2.648\n",
      "loss for epoch 0: 2.439\n",
      "Validation score:  0.453125\n",
      "loss for 0/3125: 2.330\n",
      "loss for 50/3125: 2.864\n",
      "loss for 100/3125: 2.334\n",
      "loss for 150/3125: 2.626\n",
      "loss for 200/3125: 2.401\n",
      "loss for 250/3125: 2.320\n",
      "loss for 300/3125: 2.825\n",
      "loss for 350/3125: 2.100\n",
      "loss for 400/3125: 2.000\n",
      "loss for 450/3125: 2.454\n",
      "loss for 500/3125: 2.896\n",
      "loss for 550/3125: 2.735\n",
      "loss for 600/3125: 2.917\n",
      "loss for 650/3125: 2.279\n",
      "loss for 700/3125: 2.740\n",
      "loss for 750/3125: 2.442\n",
      "loss for 800/3125: 2.330\n",
      "loss for 850/3125: 3.108\n",
      "loss for 900/3125: 2.462\n",
      "loss for 950/3125: 2.424\n",
      "loss for 1000/3125: 2.844\n",
      "loss for 1050/3125: 2.312\n",
      "loss for 1100/3125: 2.501\n",
      "loss for 1150/3125: 2.769\n",
      "loss for 1200/3125: 2.414\n",
      "loss for 1250/3125: 2.044\n",
      "loss for 1300/3125: 2.689\n",
      "loss for 1350/3125: 2.254\n",
      "loss for 1400/3125: 2.729\n",
      "loss for 1450/3125: 2.652\n",
      "loss for 1500/3125: 2.418\n",
      "loss for 1550/3125: 2.435\n",
      "loss for 1600/3125: 2.913\n",
      "loss for 1650/3125: 2.709\n",
      "loss for 1700/3125: 2.148\n",
      "loss for 1750/3125: 2.433\n",
      "loss for 1800/3125: 2.451\n",
      "loss for 1850/3125: 2.678\n",
      "loss for 1900/3125: 2.528\n",
      "loss for 1950/3125: 2.471\n",
      "loss for 2000/3125: 2.381\n",
      "loss for 2050/3125: 2.698\n",
      "loss for 2100/3125: 2.455\n",
      "loss for 2150/3125: 2.378\n",
      "loss for 2200/3125: 2.361\n",
      "loss for 2250/3125: 1.903\n",
      "loss for 2300/3125: 3.005\n",
      "loss for 2350/3125: 2.690\n",
      "loss for 2400/3125: 1.985\n",
      "loss for 2450/3125: 2.653\n",
      "loss for 2500/3125: 2.581\n",
      "loss for 2550/3125: 2.893\n",
      "loss for 2600/3125: 2.653\n",
      "loss for 2650/3125: 2.871\n",
      "loss for 2700/3125: 2.230\n",
      "loss for 2750/3125: 2.848\n",
      "loss for 2800/3125: 2.554\n",
      "loss for 2850/3125: 2.117\n",
      "loss for 2900/3125: 2.756\n",
      "loss for 2950/3125: 2.246\n",
      "loss for 3000/3125: 2.556\n",
      "loss for 3050/3125: 2.646\n",
      "loss for 3100/3125: 2.762\n",
      "loss for epoch 1: 2.274\n",
      "Validation score:  0.484375\n",
      "loss for 0/3125: 2.385\n",
      "loss for 50/3125: 2.346\n",
      "loss for 100/3125: 2.390\n",
      "loss for 150/3125: 2.276\n",
      "loss for 200/3125: 2.193\n",
      "loss for 250/3125: 2.121\n",
      "loss for 300/3125: 3.063\n",
      "loss for 350/3125: 2.926\n",
      "loss for 400/3125: 3.266\n",
      "loss for 450/3125: 2.900\n",
      "loss for 500/3125: 2.307\n",
      "loss for 550/3125: 2.685\n",
      "loss for 600/3125: 2.207\n",
      "loss for 650/3125: 2.074\n",
      "loss for 700/3125: 2.454\n",
      "loss for 750/3125: 2.103\n",
      "loss for 800/3125: 2.690\n",
      "loss for 850/3125: 2.489\n",
      "loss for 900/3125: 1.944\n",
      "loss for 950/3125: 2.597\n",
      "loss for 1000/3125: 2.576\n",
      "loss for 1050/3125: 2.615\n",
      "loss for 1100/3125: 2.057\n",
      "loss for 1150/3125: 2.346\n",
      "loss for 1200/3125: 2.551\n",
      "loss for 1250/3125: 2.311\n",
      "loss for 1300/3125: 2.066\n",
      "loss for 1350/3125: 2.075\n",
      "loss for 1400/3125: 3.159\n",
      "loss for 1450/3125: 1.632\n",
      "loss for 1500/3125: 2.559\n",
      "loss for 1550/3125: 2.110\n",
      "loss for 1600/3125: 2.212\n",
      "loss for 1650/3125: 2.520\n",
      "loss for 1700/3125: 2.392\n",
      "loss for 1750/3125: 2.576\n",
      "loss for 1800/3125: 2.733\n",
      "loss for 1850/3125: 2.457\n",
      "loss for 1900/3125: 2.509\n",
      "loss for 1950/3125: 2.589\n",
      "loss for 2000/3125: 2.863\n",
      "loss for 2050/3125: 2.532\n",
      "loss for 2100/3125: 2.799\n",
      "loss for 2150/3125: 2.491\n",
      "loss for 2200/3125: 2.029\n",
      "loss for 2250/3125: 3.158\n",
      "loss for 2300/3125: 2.540\n",
      "loss for 2350/3125: 2.055\n",
      "loss for 2400/3125: 1.937\n",
      "loss for 2450/3125: 2.114\n",
      "loss for 2500/3125: 2.366\n",
      "loss for 2550/3125: 2.403\n",
      "loss for 2600/3125: 2.012\n",
      "loss for 2650/3125: 2.331\n",
      "loss for 2700/3125: 2.242\n",
      "loss for 2750/3125: 2.541\n",
      "loss for 2800/3125: 2.049\n",
      "loss for 2850/3125: 2.104\n",
      "loss for 2900/3125: 2.238\n",
      "loss for 2950/3125: 2.208\n",
      "loss for 3000/3125: 2.280\n",
      "loss for 3050/3125: 2.338\n",
      "loss for 3100/3125: 1.575\n",
      "loss for epoch 2: 2.181\n",
      "Validation score:  0.59375\n",
      "loss for 0/3125: 2.548\n",
      "loss for 50/3125: 2.232\n",
      "loss for 100/3125: 2.013\n",
      "loss for 150/3125: 2.090\n",
      "loss for 200/3125: 2.190\n",
      "loss for 250/3125: 2.202\n",
      "loss for 300/3125: 2.419\n",
      "loss for 350/3125: 2.767\n",
      "loss for 400/3125: 2.069\n",
      "loss for 450/3125: 2.155\n",
      "loss for 500/3125: 2.654\n",
      "loss for 550/3125: 2.390\n",
      "loss for 600/3125: 1.958\n",
      "loss for 650/3125: 1.673\n",
      "loss for 700/3125: 2.034\n",
      "loss for 750/3125: 2.185\n",
      "loss for 800/3125: 1.825\n",
      "loss for 850/3125: 1.714\n",
      "loss for 900/3125: 2.429\n",
      "loss for 950/3125: 2.339\n",
      "loss for 1000/3125: 2.039\n",
      "loss for 1050/3125: 1.994\n",
      "loss for 1100/3125: 2.177\n",
      "loss for 1150/3125: 1.884\n",
      "loss for 1200/3125: 1.942\n",
      "loss for 1250/3125: 2.286\n",
      "loss for 1300/3125: 2.054\n",
      "loss for 1350/3125: 1.908\n",
      "loss for 1400/3125: 2.371\n",
      "loss for 1450/3125: 2.464\n",
      "loss for 1500/3125: 2.066\n",
      "loss for 1550/3125: 2.787\n",
      "loss for 1600/3125: 2.342\n",
      "loss for 1650/3125: 2.099\n",
      "loss for 1700/3125: 2.279\n",
      "loss for 1750/3125: 2.122\n",
      "loss for 1800/3125: 2.350\n",
      "loss for 1850/3125: 2.940\n",
      "loss for 1900/3125: 2.015\n",
      "loss for 1950/3125: 1.598\n",
      "loss for 2000/3125: 1.950\n",
      "loss for 2050/3125: 2.021\n",
      "loss for 2100/3125: 2.212\n",
      "loss for 2150/3125: 1.995\n",
      "loss for 2200/3125: 1.901\n",
      "loss for 2250/3125: 2.231\n",
      "loss for 2300/3125: 2.020\n",
      "loss for 2350/3125: 1.898\n",
      "loss for 2400/3125: 1.833\n",
      "loss for 2450/3125: 2.258\n",
      "loss for 2500/3125: 2.279\n",
      "loss for 2550/3125: 2.012\n",
      "loss for 2600/3125: 2.069\n",
      "loss for 2650/3125: 2.656\n",
      "loss for 2700/3125: 2.004\n",
      "loss for 2750/3125: 2.288\n",
      "loss for 2800/3125: 2.243\n",
      "loss for 2850/3125: 1.971\n",
      "loss for 2900/3125: 2.156\n",
      "loss for 2950/3125: 2.174\n",
      "loss for 3000/3125: 1.771\n",
      "loss for 3050/3125: 1.858\n",
      "loss for 3100/3125: 2.397\n",
      "loss for epoch 3: 2.391\n",
      "Validation score:  0.71875\n",
      "loss for 0/3125: 2.223\n",
      "loss for 50/3125: 2.168\n",
      "loss for 100/3125: 1.880\n",
      "loss for 150/3125: 2.427\n",
      "loss for 200/3125: 2.465\n",
      "loss for 250/3125: 2.239\n",
      "loss for 300/3125: 2.386\n",
      "loss for 350/3125: 2.298\n",
      "loss for 400/3125: 2.059\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7fadbc03c96f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mjc/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munless\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mvolatile\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \"\"\"\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mjc/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "# for epoch in range(epochs):\n",
    "for epoch in range(10):\n",
    "    random.shuffle(lines) # shuffle lines\n",
    "    for i in range(num_batches):\n",
    "#     for i in range(5):\n",
    "        opt.zero_grad()\n",
    "#         print(i)\n",
    "        batch = lines[i*batch_size:(i+1)*batch_size]\n",
    "        S,Q,A = from_batch(batch)\n",
    "        out = rn(S,Q)\n",
    "        A = Variable(torch.LongTensor(A))\n",
    "        if torch.cuda.is_available():\n",
    "            A = A.cuda()\n",
    "        loss = criterion(out,A)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if i % 50==0:\n",
    "            print(\"loss for %d/%d: %1.3f\" % (i,num_batches,loss.data[0]))\n",
    "    print(\"loss for epoch %d: %1.3f\" % (epoch,loss.data[0]))\n",
    "    model_name = 'rn_%d.pth' % (epoch+1+startoff)\n",
    "    if i % 5==0:\n",
    "        torch.save(obj=rn,f='saved/'+model_name)\n",
    "    total, correct = validate(rn, val_set)\n",
    "    print(\"Validation score: \",correct*1.0/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# validation\n",
    "# load dataset\n",
    "with open('test_10k.txt') as f:\n",
    "    lines=f.readlines()\n",
    "num_batches = int(len(lines)/batch_size)\n",
    "\n",
    "# load vocabulary\n",
    "word2idx = np.load('word2idx.npy').item()\n",
    "idx2word = np.load('idx2word.npy').item()\n",
    "\n",
    "rn = torch.load('saved/rn_4.pth')\n",
    "if torch.cuda.is_available():\n",
    "    rn = rn.cuda()\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "total = 0\n",
    "correct = 0\n",
    "for epoch in range(1):\n",
    "    random.shuffle(lines) # shuffle lines\n",
    "    for i in range(num_batches):\n",
    "#     for i in range(5):\n",
    "        opt.zero_grad()\n",
    "#         print(i)\n",
    "        batch = lines[i*batch_size:(i+1)*batch_size]\n",
    "        S,Q,A = from_batch(batch)\n",
    "        out = rn(S,Q)\n",
    "        # out : [batch x vocab_size]\n",
    "        O = torch.max(out,1)[1].cpu().data.numpy().squeeze()\n",
    "        score = np.array(O==A, int)\n",
    "        total+=len(score)\n",
    "        correct+=sum(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = np.array(O.squeeze()==A, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(S[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
